{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ad10a6-3276-41a6-94d4-05364835497c",
   "metadata": {},
   "source": [
    "### visulaizing the  corine images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f9a31",
   "metadata": {},
   "source": [
    "### The corine land cover classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760bd90b-70a6-4959-991c-7faa9d7b1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "clcc ={111:'cont. urban fabric',\n",
    " 112:'disc urban fabric',\n",
    " 121:'industrial or commercial units',\n",
    " 122:'road and rail',\n",
    " 123:'port areas',\n",
    " 124:'airports',\n",
    " 131:'mineral extraction sites',\n",
    " 132:'dump sites',\n",
    " 133:'construction sites',\n",
    " 141:'green urban areas',\n",
    " 142:'sport and leasure',\n",
    " 211:'non irregated arable land',\n",
    " 212:'permenant irregated land',\n",
    " 213:'rice fields',\n",
    " 221:'vine yards',\n",
    " 223:'olive groves',\n",
    " 231:'pastures',\n",
    " 241:'annual with perm. crops',\n",
    " 242:'complex cultivation patters',\n",
    " 243:'land principally occupied by agriculture',\n",
    " 244:'agro forest areas',\n",
    " 311:'broad leaved forest',\n",
    " 312:'conferous forest',\n",
    " 313:'mixed forest',\n",
    " 321:'natural grassland',\n",
    " 322:'moors and heathland',\n",
    " 323: 'scierohllous vegitation',\n",
    " 324:'transitional woodland shrub',\n",
    " 331: 'beaches dunes and sand plains',\n",
    " 332:'bare rock',\n",
    " 333:'sparsely vegetated areas',\n",
    " 334:'burnt areas',\n",
    " 335:'glaciers and perpetual snow',\n",
    " 411:'inland marshes',\n",
    " 412:'peat bogs',\n",
    " 421:'salt marshes',\n",
    " 422:'salines',\n",
    " 423:'intertidal flats',\n",
    " 511:'water courses',\n",
    " 512:'water bodies',\n",
    " 521:'costal lagoons',\n",
    " 522:'estuaries',\n",
    " 523:'sea and ocean'}\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e10dd5-ddb6-483b-b053-7d0bd6079b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "landcovers = [key for idx, key in enumerate(clcc) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64adb4e5-338e-4639-a3de-15bf16188e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[111,\n",
       " 112,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 141,\n",
       " 142,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 221,\n",
       " 223,\n",
       " 231,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 411,\n",
       " 412,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 511,\n",
       " 512,\n",
       " 521,\n",
       " 522,\n",
       " 523]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landcovers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b43d91",
   "metadata": {},
   "source": [
    "### Algorithm to change a segmentation mask into a feature vector \n",
    "#### where each element represnts the number of pixels of each class in each segmentation mask\n",
    "#### The first part is a dataloader for the segmentation mask and the labels of wild and non wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ee2631-df09-4829-93de-0e7e6490dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 23919/23919 [00:46<00:00, 512.34it/s]\n"
     ]
    }
   ],
   "source": [
    "#final code\n",
    "# to do list is to build a transparent model to train on the classes\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.image_paths = sorted(glob.glob(os.path.join(root)+ '/*.*'))\n",
    "        #self.image_paths = (self.data['file'])\n",
    "        #print(len(self.image_paths))\n",
    "        #self.image_paths = sorted(glob.glob(os.path.join('/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs', self.image_paths)))\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        # we need to open the image as a tiff file and take the first channel representing the corine channel\n",
    "        image = tiff.imread(image_path)\n",
    "        image = image.astype('int')\n",
    "        #print(image.shape)\n",
    "        image = image[:,:,0]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file = 'infos.csv'\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "dataset = CustomDataset(csv_file,root = '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs')\n",
    "\n",
    "# Create a data loader to iterate over the dataset\n",
    "batch_size = 1\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# code the image to segmentation mask function:\n",
    "# to extract the corine channel from the path\n",
    "#print(len(data_loader))\n",
    "#print(images[0])\n",
    "# Iterate over the data loader\n",
    "array = landcovers\n",
    "feature_array = []\n",
    "target_labels = []\n",
    "for images, labels in tqdm(data_loader):\n",
    "    # Convert images to segmentation mask here using your own logic\n",
    "    #print('-------loop is working-------')\n",
    "    segmentation_mask = images\n",
    "    #print(len(images))\n",
    "    # Get the unique land cover classes present in the segmentation mask\n",
    "    land_cover_classes = torch.unique(segmentation_mask)\n",
    "\n",
    "    # Compute the number of land cover classes and the maximum number of classes expected\n",
    "    num_classes = len(land_cover_classes)\n",
    "    max_num_classes = 44  # Set the maximum number of land cover classes expected in the dataset\n",
    "    \n",
    "    \n",
    "    # create a dictionary to have the key the land cover class code and as the ni. of pixels of this specific land cover class as data\n",
    "    class_counts = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate the vector with the count of pixels for each land cover class\n",
    "    \n",
    "    for class_keys in land_cover_classes:\n",
    "        class_counts = {key: 0 for key in array}\n",
    "\n",
    "        if class_keys in landcovers: # to make sure that we only have the corine land cover classess\n",
    "            \n",
    "\n",
    "        \n",
    "            class_rep = torch.sum(segmentation_mask == class_keys)\n",
    "            #class_keys = class_keys.item\n",
    "            #class_rep = class_rep.item\n",
    "\n",
    "            class_counts[class_keys.item()] = class_rep.item()\n",
    "            #print(class_counts)\n",
    "            #time.sleep(1)\n",
    "            \n",
    "    #print('the label is -->',labels.item())\n",
    "    # Trim the class_counts tensor to the actual number of classes found\n",
    "    #class_counts = class_counts[:num_classes]\n",
    "    \n",
    "    # Print the class counts vector\n",
    "    '''print(\"Class Counts:\")\n",
    "    print(class_counts.values())\n",
    "   \n",
    "    \n",
    "    print('feature_array len-->',len(feature_array),'target_labels len-->',len(target_labels))\n",
    "    print('feature_array -->',(feature_array),'target_labels -->',(target_labels))\n",
    "    #print(labels)\n",
    "    #time.sleep(0)'''\n",
    "    \n",
    "    \n",
    "    feature_array.append(list(class_counts.values()))\n",
    "    target_labels.append(labels.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec1faa",
   "metadata": {},
   "source": [
    "### Dataset and dataloader for the scene classification dataset (Anthroprotect dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265d734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import tifffile as tiff\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.images = self.data['file']\n",
    "        self.labels = self.data['label']\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]  # Assuming the image names are in the first column of the CSV file\n",
    "        image_label = self.labels[index]\n",
    "        # Construct the complete image path by joining the folder path and image name\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "        # Open the image using PIL\n",
    "        image = tiff.imread(image_path)\n",
    "        \n",
    "        #choos ethe number of channels you need\n",
    "        image= image[:,:,:3]\n",
    "        #####\n",
    "        \n",
    "        image = image.astype('uint8')\n",
    "        #image = image.astype('int')\n",
    "        # Apply transformations, if provided\n",
    "        if self.transform:\n",
    "            #pil_image = Image.fromarray(image)\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        return image, image_label\n",
    "    \n",
    "    \n",
    "csv_file =  'infos.csv'\n",
    "image_folder = '~/working_folder/data/anthroprotect/tiles/s2'\n",
    "transform = transforms.ToTensor()  # Example transformation, you can add more\n",
    "\n",
    "dataset = CustomDataset(csv_file, image_folder, transform=transform) # Replace with your actual dataset instantiation\n",
    "\n",
    "# Define the ratio of the dataset to be used for testing\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the number of samples for testing\n",
    "test_size = int(test_ratio * len(dataset))\n",
    "\n",
    "# Calculate the number of samples for training\n",
    "train_size = len(dataset) - test_size\n",
    "\n",
    "# Split the dataset into training and testing datasets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader instances for training and testing datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349b39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),  # Resize the image to (224, 224)\n",
    "    transforms.ToTensor()  # Convert the PIL Image to tensor\n",
    "])\n",
    "scene_dataset= CustomDataset(csv_file, '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2',transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf52b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23919"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128  # Define the batch size\n",
    "data_loader = DataLoader(scene_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a08b7",
   "metadata": {},
   "source": [
    "### a classifier for scence classification, with a method to extract a the embeddings from the bottelneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1356f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(256 * 16 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(10,1),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        flattened = torch.flatten(features, start_dim=1)\n",
    "        embedding = self.embedding(flattened)\n",
    "        output = self.classifier(embedding)\n",
    "        \n",
    "        return output, embedding\n",
    "\n",
    "# Example usage\n",
    "input_channels = 3\n",
    "image_size = 256\n",
    "num_classes = 1\n",
    "model = DeepClassifier(num_classes)\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Training the model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22652b22",
   "metadata": {},
   "source": [
    "### Training of the scene classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d964b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|█▎                       | 1/20 [01:00<19:03, 60.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  10%|██▌                      | 2/20 [02:01<18:11, 60.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  15%|███▊                     | 3/20 [03:00<17:04, 60.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  20%|█████                    | 4/20 [04:01<16:05, 60.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  25%|██████▎                  | 5/20 [05:01<15:05, 60.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  30%|███████▌                 | 6/20 [06:01<14:04, 60.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  35%|████████▊                | 7/20 [07:02<13:05, 60.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  40%|██████████               | 8/20 [08:03<12:05, 60.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  45%|███████████▎             | 9/20 [09:03<11:03, 60.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  50%|████████████            | 10/20 [10:05<10:09, 60.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  55%|█████████████▏          | 11/20 [11:06<09:09, 61.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  60%|██████████████▍         | 12/20 [12:09<08:13, 61.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  65%|███████████████▌        | 13/20 [13:14<07:17, 62.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  70%|████████████████▊       | 14/20 [14:17<06:16, 62.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  75%|██████████████████      | 15/20 [15:20<05:13, 62.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  80%|███████████████████▏    | 16/20 [16:25<04:13, 63.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  85%|████████████████████▍   | 17/20 [17:29<03:10, 63.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  90%|█████████████████████▌  | 18/20 [18:33<02:07, 63.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Progress:  95%|██████████████████████▊ | 19/20 [19:38<01:03, 63.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████| 20/20 [20:42<00:00, 62.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 20\n",
    "# Assuming you have your training data in a PyTorch DataLoader called \"train_loader\"\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "    # Variables to keep track of accuracy and total samples\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for data in data_loader:\n",
    "        inputs, labels = data  \n",
    "        labels = labels.float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, encoded_data = model(inputs.float())\n",
    "        \n",
    "        \n",
    "        \n",
    "        labels = labels.view(-1, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        total_samples += inputs.size(0)\n",
    "        \n",
    "    mse = total_loss / total_samples\n",
    "    print(f\"Training MSE: {mse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ca480",
   "metadata": {},
   "source": [
    "### Logistic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transparent model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#add the mbedding here\n",
    "\n",
    "X = feature_array. append(embedding)\n",
    "\n",
    "\n",
    "##get the target values from the classifier\n",
    "target = target_labels\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale the input features\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "logreg.fit(X_scaled, target)\n",
    "\n",
    "# Print the coefficients of the trained model\n",
    "coefficients = logreg.coef_[0]\n",
    "for feature, coefficient in zip(clcc.keys(), coefficients):\n",
    "    print(f\"{feature}: {coefficient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a86779",
   "metadata": {},
   "source": [
    "### visulaization the coefficients of the LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the coefficients of the trained transparent model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a trained logistic regression model named `logreg`\n",
    "coefficients = coefficients\n",
    "feature_names = list(clcc.values())  # Assuming `input_data` contains the feature names\n",
    "\n",
    "# Sort the coefficients and feature names in descending order of absolute magnitude\n",
    "sorted_indices = np.argsort(np.abs(coefficients))[::-1]\n",
    "sorted_coefficients = coefficients[sorted_indices]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 16))\n",
    "plt.barh(range(len(sorted_coefficients)), sorted_coefficients, align='center')\n",
    "plt.yticks(range(len(sorted_coefficients)), sorted_feature_names)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Importance of Coefficients in Logistic Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97110bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a trained logistic regression model named `logreg`\n",
    "\n",
    "  # Assuming `input_data` contains the feature names\n",
    "\n",
    "# Sort the coefficients and feature names in descending order of absolute magnitude\n",
    "sorted_indices = np.argsort(np.abs(coefficients))[::-1]\n",
    "sorted_coefficients = coefficients[sorted_indices]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# Separate positive and negative coefficients\n",
    "positive_indices = np.where(sorted_coefficients >= 0)[0]\n",
    "negative_indices = np.where(sorted_coefficients < 0)[0]\n",
    "\n",
    "positive_coefficients = sorted_coefficients[positive_indices]\n",
    "positive_feature_names = [sorted_feature_names[i] for i in positive_indices]\n",
    "\n",
    "negative_coefficients = sorted_coefficients[negative_indices]\n",
    "negative_feature_names = [sorted_feature_names[i] for i in negative_indices]\n",
    "\n",
    "# Create separate plots for positive and negative coefficients using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Positive coefficients plot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=positive_coefficients, y=positive_feature_names)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Positive Coefficients')#\n",
    "\n",
    "# Negative coefficients plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=negative_coefficients, y=negative_feature_names)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Negative Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a618d",
   "metadata": {},
   "source": [
    "### Fine tunning pretrained segmentation model deeplapv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-10_177.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_905_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_113.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-4_13.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_43.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-8_104.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_78.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.82742-62.40360_36.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_824_40.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-13_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_12.86738-59.13588_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-16_123.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64504_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_148.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-6_82.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-18_171.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-1_75.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_17.28151-62.39233_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-14_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_7.56068-58.29100_15.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_210.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-5_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-8_153.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_90.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-21_155.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-15_108.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_136.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-15_29.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64502_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_555633109_0.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-9_33.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-6_27.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174625_57.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-2_117.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633097_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_152.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64507_103.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_111.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-7_79.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_3.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_199.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174617_51.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_17.28151-62.39233_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-02-8_16.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-0_10.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-8_45.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_162.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_166.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-10_104.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_106840_54.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-3_50.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_105.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_1517_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.19332-61.07551_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-8_68.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-9_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-8_46.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555639726_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-6_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_171.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_239.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-11_53.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_123.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-9_30.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-23_8.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_89.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-15_25.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-1_8.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_21.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-0_6.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64508_156.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-7_52.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-6_116.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64507_160.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_58.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_64.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-7_20.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.80068-61.49478_51.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-6_160.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-2_18.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_182936_2.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_125.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-9_26.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_112.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-11_13.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_195.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-1_69.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_52.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_263.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-4_29.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_70.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174626_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_119.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-20_13.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174625_37.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-1_33.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-2_30.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_81.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-6_29.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-11_21.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_204.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_153062_11.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-6_2.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-7_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-2_112.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.06581-62.07804_6.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_825_9.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-12_42.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-4_74.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-6_61.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-5_67.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-6_36.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_164.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_19.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346054-3_67.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-13_110.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-1_18.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-1_74.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_67.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_73.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555639270_31.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_392918_19.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_12297_54.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.82742-62.40360_72.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-15_115.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_212.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-13_142.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.32262-62.80112_37.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_71.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-9_51.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_103.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64508_69.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-7_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_30792_26.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_112.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64508_119.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-15_43.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_65.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-15_151.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_5.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174624_14.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-23_38.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_98.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_165.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555710270_81.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_175.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-1_2.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.65564-56.71377_76.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-22_31.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-1_66.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-1_65.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_1517_46.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-7_40.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.78734-62.51648_42.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-7_52.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_116.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_832_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_200.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_12.86738-59.13588_85.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-10_166.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555710270_80.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_272.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-8_34.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_1517_72.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_232.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_6907_63.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633097_3.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-0_69.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-22_96.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-4_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_25.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_155.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_30768_19.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-3_15.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-4_50.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-0_16.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-20_23.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_117.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-1_89.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-21_115.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-3_37.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_101.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-3_3.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-8_50.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-5_124.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-10_157.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-1_132.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-7_152.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_113.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-10_138.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-7_51.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-1_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-10_14.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_101.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555710270_73.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_24.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_395372_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_44.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-8_47.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_1517_41.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_46.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_24.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-15_35.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_142.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.06581-62.07804_31.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-14_8.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-8_6.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-0_7.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_106837_48.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_150.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_174.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-6_102.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-1_20.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64537_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-9_96.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-1_15.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-12_56.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-5_41.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-7_44.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64506_53.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-7_61.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_114.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-12_140.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-1_64.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174617_56.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633073_26.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-1_48.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-18_16.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-7_16.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_29.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.87779-58.69351_18.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-3_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-17_3.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_102003_10.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-17_84.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-15_21.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-14_128.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-14_23.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_157.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_139.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-3_12.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-8_4.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_207.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_11.34316-63.79593_15.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-1_35.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_244.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_138.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_23.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_103.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-5_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_207.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_171.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_153.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-8_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.20696-59.31534_3.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_170.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_163.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_555552443_3.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174623_34.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-8_56.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-2_26.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_252.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64504_142.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-11_20.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-2_137.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-02-7_17.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-9_26.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-1_28.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-1_45.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-23_51.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-0_119.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-3_117.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_117.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-2_190.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_9.97900-63.24420_0.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-5_10.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-8_44.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_136.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_56.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.57677-63.28057_4.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-4_3.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-16_125.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.19332-61.07551_35.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-6_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_21.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-3_115.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_59.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555639726_11.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-0_4.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_44.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_130.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-3_48.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_10.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.65564-56.71377_114.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_40.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_240.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-8_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_905_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_163.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_69.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-1_8.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-6_152.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_16.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_88.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_12297_37.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_43.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_26.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-6_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_3.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_824_33.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-8_61.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_229.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-17_64.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633073_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-12_124.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_72.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_20.03637-63.83529_54.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_68.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-8_90.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.19332-61.07551_47.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.54354-61.68142_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174610_16.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_9.97900-63.24420_110.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-3_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_210.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-23_32.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_51.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_134.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.54354-61.68142_12.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_102.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-10_169.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-6_72.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_125.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-0_22.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_146.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_282.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_174.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633073_18.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_95.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_30.24940-62.11777_36.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-5_23.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_30.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-2_32.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-11_92.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-15_37.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174622_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_20.03637-63.83529_10.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-6_102.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_182768_14.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_191.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-13_103.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-7_92.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_77.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.82742-62.40360_76.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_321.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_259.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.87779-58.69351_26.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_43.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_2561_21.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.87779-58.69351_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_166.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-7_69.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-15_117.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-8_76.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_153.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_32.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_2561_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_117.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-1_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-10_19.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64537_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-17_22.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-1_48.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-1_32.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_239.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-3_8.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_275.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-10_139.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-0_20.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-8_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64507_28.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_42.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-6_31.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_110.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_125.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_106837_14.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_224.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_201.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_130.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-12_46.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_16.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_113.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_161.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_12297_74.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-3_4.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-1_34.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_106.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174617_63.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_112.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-8_117.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-10_74.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174616_9.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_163.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_187.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-8_2.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-13_126.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_147.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-4_44.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-4_46.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-8_12.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-13_6.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-12_53.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_84.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64536_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-8_60.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-18_112.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_102.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-2_57.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_105.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-15_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_907_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_183.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_211.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555710270_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_1517_32.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_11.34316-63.79593_69.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-8_8.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-8_172.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174621_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_907_35.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_15.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-16_4.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_12.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_118.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_906_29.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64541_5.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_131.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174625_17.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-8_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-21_42.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_40.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_24.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-11_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_30745_20.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-5_26.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_34.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_127.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-1_17.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_122.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_8.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174621_9.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_109.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-21_29.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_162.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_149.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_171.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-15_93.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_45.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-18_38.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-6_52.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-1_63.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-3_11.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174616_6.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_30.24940-62.11777_75.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-17_163.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_2561_42.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_81.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_263.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_153.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_119.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4657_16.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-17_49.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_70.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64504_156.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_86.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_316.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_193.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-13_90.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-02-8_36.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-17_186.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_155.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.65564-56.71377_115.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_826_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_74.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_2561_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-11_68.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_18948_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174617_10.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-17_72.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_829_8.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_99.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.28550-61.61286_31.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_95.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_76.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_153.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.52111-59.46264_72.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-3_58.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_33.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_40.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_148.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.19332-61.07551_138.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_47.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-02-6_14.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_14.38127-59.41614_12.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_112.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_226.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-6_56.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64504_51.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.65564-56.71377_128.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346054-3_48.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_106.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-2_90.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555639270_85.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-0_52.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-3_152.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_192.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-2_39.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-6_70.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-3_39.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_34.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.82742-62.40360_32.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_97.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-1_18.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_100.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-4_93.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_65.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-16_93.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_60.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-1_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-3_17.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-13_136.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-8_134.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_12.86738-59.13588_108.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-21_75.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-15_41.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_154.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-2_51.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-13_18.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_55.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_259.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_88.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_194.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_905_99.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_14.38127-59.41614_9.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_121.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-20_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-14_14.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_77.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_193.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_157.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_32.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_106840_53.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_193473_11.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_82.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-21_84.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64506_116.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-4_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-17_13.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_13.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_6907_143.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-8_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-3_92.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-1_31.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-13_98.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174609_17.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.28550-61.61286_6.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-4_19.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.82742-62.40360_10.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_168.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-2_165.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-7_34.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-11_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-1_20.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-8_63.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_79.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-6_74.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_9.97900-63.24420_111.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.20696-59.31534_8.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_158.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_92.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_30745_31.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_102.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-21_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_102.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_152.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_121.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_6.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_97.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64544_16.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_905_120.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_7.56068-58.29100_34.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.94317-63.62352_4.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-5_20.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-7_146.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_149.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-0_18.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_53.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_30768_16.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-15_46.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-3_18.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_141.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_74.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_12.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_151.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_193412_9.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-5_96.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-8_17.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-10_186.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-6_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.65564-56.71377_55.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174622_64.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-5_7.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-5_21.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_190.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-10_12.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-1_22.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_105.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_2561_49.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-8_2.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_824_77.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555712752_10.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_49.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-14_224.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-10_96.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_9.97900-63.24420_74.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633073_11.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-0_41.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-13_35.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-1_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-14_107.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-0_4.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_66.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-5_29.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-12_92.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_150.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-8_113.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-0_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-9_51.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_171.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_149666_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-22_54.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_193.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-11_90.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-5_6.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-0_77.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.80068-61.49478_30.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-8_81.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.78734-62.51648_78.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_51.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-8_64.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-9_22.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.19332-61.07551_39.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174612_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_85.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_193412_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-4_54.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_55.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_124.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-6_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_116.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-8_36.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_71.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-5_63.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_104.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-6_11.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-3_11.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_158.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-22_87.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-3_11.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_106.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_74.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-0_65.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-8_88.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.19332-61.07551_109.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-22_97.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-14_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-18_179.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-1_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-16_130.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-0_26.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_20.03637-63.83529_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64504_31.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_16.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64507_10.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-12_50.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_107.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_182889_88.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-5_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_64.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174621_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_30768_89.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_151.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64543_6.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_237.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-1_6.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_81.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555712752_65.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-1_41.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-4_34.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-4_19.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-4_81.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-8_136.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_30.24940-62.11777_35.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_8.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_90.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_159.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-7_36.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_107.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_106.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-5_16.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-16_40.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.32262-62.80112_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.52111-59.46264_20.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_174.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_95.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_86.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555710270_50.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64508_157.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_17.28151-62.39233_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_2561_109.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-1_39.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.64681-61.68692_7.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_826_35.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-15_37.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_71.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-9_110.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-16_140.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-6_64.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_58.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_43.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_100.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-1_25.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_236.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64543_33.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.78734-62.51648_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174621_10.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_85.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.64681-61.68692_22.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_85.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_229.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-5_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_32.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-12_18.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-1_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346054-3_20.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-12_91.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_118.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_75.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-7_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_20.88752-64.62669_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_29.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-3_8.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_185.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-0_4.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64502_37.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_40657_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-17_11.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_205.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_116.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_1518_3.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_52.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64504_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64502_45.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-6_215.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-22_172.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_61.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3158_4.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-16_160.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_106.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.87779-58.69351_27.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-8_7.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_156.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_62123_0.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_5.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_123.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-1_52.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_67.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-1_94.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-1_32.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-1_59.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.06581-62.07804_49.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-7_154.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-7_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-8_31.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-7_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-8_27.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_108.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_106840_112.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_30.24940-62.11777_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_30.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_135.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_89.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-1_26.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_905_25.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_28.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555710270_27.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_908_3.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-6_135.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-9_56.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_121.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-10_5.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_177.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-3_9.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-14_93.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-8_160.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_7.56068-58.29100_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346054-3_68.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_161.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-4_5.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-3_9.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-8_65.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_150.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-0_42.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_395372_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-8_65.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-21_57.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-21_3.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-1_86.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_201.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-14_207.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-1_6.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-18_122.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-18_37.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-6_97.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-12_118.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64507_170.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-1_15.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-7_11.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_906_4.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-7_143.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-8_8.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64505_43.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64500_3.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-1_43.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_197.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-1_26.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_19.94464-60.22734_29.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_11.34316-63.79593_50.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.08231-56.69262-1_21.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_251.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_201.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-0_65.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_158.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_133.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.52111-59.46264_36.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-1_3.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-6_200.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-7_59.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-2_84.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-2_16.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_10.58273-61.03010-8_53.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_153062_4.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-3_32.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.32262-62.80112_31.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_180.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_120.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_151.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-1_25.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-16_114.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-9_45.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-7_37.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-0_118.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-2_124.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_12.86738-59.13588_65.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-23_44.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-06_32.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_38.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.78734-62.51648_45.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-02-7_65.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_30785_23.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-5_77.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-2_46.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_40.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_140.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_6907_119.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_192.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_82.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_907_58.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-5_80.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-14_82.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_28.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-3_10.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-12_121.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-0_45.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-17_138.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-16_36.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_161.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_17.28151-62.39233_20.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-8_66.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_4.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_206.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_91.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-21_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-1_34.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_1517_48.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.87779-58.69351_29.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-1_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_65.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_162.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_34.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_905_98.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_34.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174624_38.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-22_94.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-6_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346054-3_62.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174622_58.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-0_58.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-21_118.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-15_48.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-1_48.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-5_87.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_249.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.78734-62.51648_72.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_100.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-12_82.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-1_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-23_16.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64502_67.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174617_64.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_61.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-3_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_219.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_110.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-13_14.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-12_152.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633097_17.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-0_29.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_654_116.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_57.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_122.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.82742-62.40360_71.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64506_158.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_171.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-16_58.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-7_26.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-5_86.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-4_15.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-8_13.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_106840_87.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-02-7_68.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64506_100.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-0_18.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_115.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_127.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_173.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_824_46.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-13_222.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-7_10.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_106840_82.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-1_99.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_79.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-1_15.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-23_7.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-2_14.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_57.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-13_27.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-0_54.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64507_94.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-15_51.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64502_48.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_12.86738-59.13588_47.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_12297_13.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_195.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_15.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_35.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-3_50.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-23_75.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-17_171.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-5_42.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-15_68.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-17_27.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555710270_39.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_829_5.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-9_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-10_8.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.06581-62.07804_111.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_822_33.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-6_41.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_191.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_62104_2.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_221.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_230.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_79.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-20_3.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-6_0.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-2_152.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.82742-62.40360_21.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-0_38.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_11.34316-63.79593_4.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_11.34316-63.79593_65.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_177.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-14_129.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-16_132.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633097_33.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-15_93.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_142.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_198.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_48.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-1_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-21_5.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.06581-62.07804_114.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-8_99.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-02-7_74.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64508_95.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-9_134.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_193473_6.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-3_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-8_14.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_166.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-2_47.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174617_49.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-15_34.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ia_1517_53.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-12_38.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-10_73.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-17_166.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_103.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.65564-56.71377_131.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_218.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64507_68.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.65564-56.71377_143.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-10_22.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-8_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-04-3_77.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174624_44.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.52111-59.46264_9.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-18_89.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_11.34316-63.79593_27.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-1_107.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-1_116.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-13_130.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_149.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_17.28151-62.39233_17.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_905_76.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346056_197.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-11_195.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-1_30.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555639270_3.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_12297_6.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555710270_66.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64543_9.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64506_73.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64504_69.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64503_70.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_155143_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-14_9.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_555633053_7.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_87.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-16_211.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-5_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-19_86.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-17_62.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.49914-59.47907_75.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-2_60.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-3_29.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_24.81681-64.10019-5_25.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-21_19.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_20.88752-64.62669_42.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_346054-3_24.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64504_67.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_28.06581-62.07804_37.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_174624_26.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_155143_28.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_42.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64509-2_44.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-7_139.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_215.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_158.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-11_77.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_20.03637-63.83529_28.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-13_21.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.32262-62.80112_8.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_18.48087-57.46266_213.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_25.28673-64.74203-3_36.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-18_65.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-06_33.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_5.87779-58.69351_75.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_3159-4_146.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-01-16_148.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-9_15.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_64540_4.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_166.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-14_47.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_828-3_3.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-14_60.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_19.94464-60.22734_1.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_905_93.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-01-13_167.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-1_2.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-11_66.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-II_4658-1_61.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_27.27446-63.38494-8_65.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-03-2_189.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_30768_86.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_29.78734-62.51648_1.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-04-6_86.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-6_51.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_16.87133-59.52749-14_39.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n",
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_13.61147-57.69620-05-5_191.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/wdpa-Ib_106840_95.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models output shape---> torch.Size([2, 1, 512, 512])\n",
      "target shape---> torch.Size([2, 1, 512, 512])\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_20.03637-63.83529_40.tif\n",
      "(256, 256)\n",
      "/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2/anthropo_23.70633-61.43592-03-8_86.tif\n",
      "(256, 256)\n",
      "input image shape---> torch.Size([2, 3, 512, 512])\n",
      "ground truth target shape-----> torch.Size([2, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import segmentation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import tifffile as tiff\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, target_folder,transform_image=None,transform_target=None):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        \n",
    "        #the scene input images and the target segmentation maps both have the same name but in different folder\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.image_paths = self.data['file']\n",
    "        \n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.target_folder = target_folder\n",
    "        \n",
    "        \n",
    "        self.transform = transform\n",
    "        self.transform_target = transform_target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "  \n",
    "\n",
    "        \n",
    "        # Construct the complete image path by joining the folder path and image name\n",
    "        image_name = self.image_paths[index] \n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "        print(image_path)\n",
    "        target_path = os.path.join(self.target_folder, image_name)\n",
    "        # Open the image using PIL\n",
    "        image = tiff.imread(image_path)\n",
    "        target = tiff.imread(target_path)\n",
    "        \n",
    "        #choos ethe number of channels you need\n",
    "        image= image[:,:,:3]\n",
    "        target.shape\n",
    "        target= target [:,:,0]\n",
    "        #####\n",
    "        print(target.shape)\n",
    "        \n",
    "        image = image.astype('uint8')\n",
    "        target = target.astype('uint8')\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            \n",
    "            image = self.transform(image)\n",
    "            target = self.transform_target(target)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    #-----------------------------------------------------------------------------------#\n",
    "\n",
    "# Define the paths to your training data\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model and set it to training mode\n",
    "\n",
    "model = segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[-1] = torch.nn.Conv2d(256, 1, kernel_size=1)\n",
    "model.train()\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the transformation for input images and labels\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    \n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "])\n",
    "transform_target =transforms.Compose([transforms.ToPILImage(),transforms.Resize((512, 512)),\n",
    "                                         transforms.ToTensor()])\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "csv_file =  'infos.csv'\n",
    "image_folder = '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2'\n",
    "target_folder='/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs'\n",
    "    \n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(csv_file, image_folder, target_folder,transform_image=transform, transform_target =transform_target)\n",
    "\n",
    "# Create the data loader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in dataloader:\n",
    "        print('input image shape--->',images.shape)\n",
    "        print('ground truth target shape----->',targets.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        print('models output shape--->',outputs['out'].shape)\n",
    "        print('target shape--->',targets.shape)\n",
    "        \n",
    "        loss = criterion(outputs['out'], targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), 'fine_tuned_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5df63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
